{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk, re, time\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import chars2vec\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, GlobalMaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('/Users/admin/Documents/Queens_Masters_Courses/Deep_Learning/course_project/best/pre_processed_steam_reviews_final2.csv')\n",
    "\n",
    "# Shuffle data and remove test set\n",
    "X_data, X_test, y_data, y_test = train_test_split(\n",
    "    data['content'].values, \n",
    "    data['recommend'].values, \n",
    "    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n"
     ]
    }
   ],
   "source": [
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_LENGTH = max([len(s.split()) for s in data['content'].values.tolist()])\n",
    "word2vec_path = 'test/reviews_embedding_word2vec_recent.txt'\n",
    "\n",
    "\n",
    "# Create a 2D vector of tokenized words\n",
    "data_lines = [word_tokenize(line) for line in data['content'].values.tolist()]\n",
    "\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(data_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(data_lines)\n",
    "data_word_index = tokenizer_obj.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  49136\n"
     ]
    }
   ],
   "source": [
    "## Create Word2Vec model\n",
    "word2vec_model = create_word2vec_model(data_lines, EMBEDDING_DIM)\n",
    "\n",
    "# Get embeddings index\n",
    "embeddings_index = get_embeddings_index(word2vec_model, word2vec_path)\n",
    "\n",
    "# Create embedding matrix\n",
    "num_words, embedding_matrix = create_embedding_matrix(EMBEDDING_DIM, data_word_index, embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('incredible', 0.9616700410842896),\n",
       " ('fantastic', 0.9353382587432861),\n",
       " ('brilliant', 0.9319596290588379),\n",
       " ('stunning', 0.9298729300498962),\n",
       " ('outstanding', 0.929619312286377),\n",
       " ('wonderful', 0.92575603723526),\n",
       " ('gorgeous', 0.9215208292007446),\n",
       " ('immersive', 0.9208390116691589),\n",
       " ('stellar', 0.9030176401138306),\n",
       " ('rich', 0.8990558981895447)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test word2vec\n",
    "word2vec_model.wv.most_similar('excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/5...\n",
      "Found 38906 unique tokens.\n",
      "Found 19116 unique tokens.\n",
      "Shape of X_train_pad tensor: (29547, 770)\n",
      "Shape of y_train tensor: (29547,)\n",
      "Shape of X_val_pad tensor: (7388, 770)\n",
      "Shape of y_val tensor: (7388,)\n",
      "====================\n",
      "Building model...\n",
      "{0: 1.9342105263157894, 1: 0.6743119266055045}\n",
      "Training model...\n",
      "Train on 29547 samples, validate on 7388 samples\n",
      "Epoch 1/30\n",
      " - 358s - loss: 0.6446 - acc: 0.6015 - val_loss: 0.6072 - val_acc: 0.6631\n",
      "Epoch 2/30\n",
      " - 348s - loss: 0.5705 - acc: 0.6821 - val_loss: 0.7459 - val_acc: 0.5421\n",
      "Epoch 3/30\n",
      " - 344s - loss: 0.5440 - acc: 0.7020 - val_loss: 0.7475 - val_acc: 0.5704\n",
      "Epoch 4/30\n",
      " - 341s - loss: 0.5294 - acc: 0.7176 - val_loss: 0.7658 - val_acc: 0.5562\n",
      "Epoch 5/30\n",
      " - 343s - loss: 0.5160 - acc: 0.7261 - val_loss: 0.7098 - val_acc: 0.6034\n",
      "Epoch 6/30\n",
      " - 343s - loss: 0.5089 - acc: 0.7349 - val_loss: 0.7574 - val_acc: 0.5726\n",
      "Epoch 7/30\n",
      " - 342s - loss: 0.4998 - acc: 0.7393 - val_loss: 0.8129 - val_acc: 0.5575\n",
      "Epoch 8/30\n",
      " - 342s - loss: 0.4959 - acc: 0.7461 - val_loss: 0.7423 - val_acc: 0.5897\n",
      "Epoch 9/30\n",
      " - 343s - loss: 0.4877 - acc: 0.7522 - val_loss: 0.7982 - val_acc: 0.5659\n",
      "Epoch 10/30\n",
      " - 342s - loss: 0.4822 - acc: 0.7565 - val_loss: 0.7405 - val_acc: 0.5989\n",
      "Epoch 11/30\n",
      " - 343s - loss: 0.4801 - acc: 0.7583 - val_loss: 0.7160 - val_acc: 0.6106\n",
      "Last training accuracy:0.7582834128575482, validation accuracy: 0.6105847319655633\n",
      "Training on fold 2/5...\n",
      "Found 39048 unique tokens.\n",
      "Found 19022 unique tokens.\n",
      "Shape of X_train_pad tensor: (29547, 770)\n",
      "Shape of y_train tensor: (29547,)\n",
      "Shape of X_val_pad tensor: (7388, 770)\n",
      "Shape of y_val tensor: (7388,)\n",
      "====================\n",
      "Building model...\n",
      "{0: 1.9342105263157894, 1: 0.6743119266055045}\n",
      "Training model...\n",
      "Train on 29547 samples, validate on 7388 samples\n",
      "Epoch 1/30\n",
      " - 340s - loss: 0.6544 - acc: 0.5933 - val_loss: 0.6197 - val_acc: 0.6420\n",
      "Epoch 2/30\n",
      " - 335s - loss: 0.5763 - acc: 0.6833 - val_loss: 0.7011 - val_acc: 0.5774\n",
      "Epoch 3/30\n",
      " - 334s - loss: 0.5460 - acc: 0.7100 - val_loss: 0.6775 - val_acc: 0.6076\n",
      "Epoch 4/30\n",
      " - 335s - loss: 0.5298 - acc: 0.7225 - val_loss: 0.7096 - val_acc: 0.5964\n",
      "Epoch 5/30\n",
      " - 335s - loss: 0.5190 - acc: 0.7302 - val_loss: 0.7009 - val_acc: 0.6003\n",
      "Epoch 6/30\n",
      " - 335s - loss: 0.5091 - acc: 0.7383 - val_loss: 0.7440 - val_acc: 0.5782\n",
      "Epoch 7/30\n",
      " - 337s - loss: 0.5005 - acc: 0.7454 - val_loss: 0.7812 - val_acc: 0.5531\n",
      "Epoch 8/30\n",
      " - 336s - loss: 0.4983 - acc: 0.7481 - val_loss: 0.7181 - val_acc: 0.6038\n",
      "Epoch 9/30\n",
      " - 335s - loss: 0.4888 - acc: 0.7549 - val_loss: 0.7371 - val_acc: 0.5910\n",
      "Epoch 10/30\n",
      " - 337s - loss: 0.4802 - acc: 0.7589 - val_loss: 0.8005 - val_acc: 0.5470\n",
      "Epoch 11/30\n",
      " - 339s - loss: 0.5123 - acc: 0.7414 - val_loss: 0.7324 - val_acc: 0.6018\n",
      "Last training accuracy:0.7413950654848476, validation accuracy: 0.6017866811044937\n",
      "Training on fold 3/5...\n",
      "Found 38834 unique tokens.\n",
      "Found 19075 unique tokens.\n",
      "Shape of X_train_pad tensor: (29548, 770)\n",
      "Shape of y_train tensor: (29548,)\n",
      "Shape of X_val_pad tensor: (7387, 770)\n",
      "Shape of y_val tensor: (7387,)\n",
      "====================\n",
      "Building model...\n",
      "{0: 1.9342759884786593, 1: 0.6743039707895938}\n",
      "Training model...\n",
      "Train on 29548 samples, validate on 7387 samples\n",
      "Epoch 1/30\n",
      " - 333s - loss: 0.6611 - acc: 0.5819 - val_loss: 0.7754 - val_acc: 0.4725\n",
      "Epoch 2/30\n",
      " - 331s - loss: 0.5896 - acc: 0.6682 - val_loss: 0.6383 - val_acc: 0.6176\n",
      "Epoch 3/30\n",
      " - 333s - loss: 0.5548 - acc: 0.6957 - val_loss: 0.6167 - val_acc: 0.6455\n",
      "Epoch 4/30\n",
      " - 332s - loss: 0.5366 - acc: 0.7102 - val_loss: 0.6645 - val_acc: 0.6157\n",
      "Epoch 5/30\n",
      " - 332s - loss: 0.5260 - acc: 0.7235 - val_loss: 0.6829 - val_acc: 0.6019\n",
      "Epoch 6/30\n",
      " - 334s - loss: 0.5117 - acc: 0.7317 - val_loss: 0.6441 - val_acc: 0.6413\n",
      "Epoch 7/30\n",
      " - 331s - loss: 0.5080 - acc: 0.7362 - val_loss: 0.6726 - val_acc: 0.6176\n",
      "Epoch 8/30\n",
      " - 332s - loss: 0.4996 - acc: 0.7402 - val_loss: 0.6651 - val_acc: 0.6280\n",
      "Epoch 9/30\n",
      " - 330s - loss: 0.4913 - acc: 0.7500 - val_loss: 0.7142 - val_acc: 0.5912\n",
      "Epoch 10/30\n",
      " - 334s - loss: 0.4903 - acc: 0.7502 - val_loss: 0.7421 - val_acc: 0.5737\n",
      "Epoch 11/30\n",
      " - 331s - loss: 0.4854 - acc: 0.7521 - val_loss: 0.6978 - val_acc: 0.6071\n",
      "Epoch 12/30\n",
      " - 332s - loss: 0.4836 - acc: 0.7538 - val_loss: 0.6713 - val_acc: 0.6326\n",
      "Epoch 13/30\n",
      " - 331s - loss: 0.4781 - acc: 0.7589 - val_loss: 0.7134 - val_acc: 0.5997\n",
      "Last training accuracy:0.7589007716500358, validation accuracy: 0.5997021795206727\n",
      "Training on fold 4/5...\n",
      "Found 39085 unique tokens.\n",
      "Found 18736 unique tokens.\n",
      "Shape of X_train_pad tensor: (29549, 770)\n",
      "Shape of y_train tensor: (29549,)\n",
      "Shape of X_val_pad tensor: (7386, 770)\n",
      "Shape of y_val tensor: (7386,)\n",
      "====================\n",
      "Building model...\n",
      "{0: 1.9340882314439063, 1: 0.6743267914194432}\n",
      "Training model...\n",
      "Train on 29549 samples, validate on 7386 samples\n",
      "Epoch 1/30\n",
      " - 330s - loss: 0.6514 - acc: 0.5892 - val_loss: 0.5843 - val_acc: 0.6538\n",
      "Epoch 2/30\n",
      " - 330s - loss: 0.5776 - acc: 0.6736 - val_loss: 0.6091 - val_acc: 0.6351\n",
      "Epoch 3/30\n",
      " - 330s - loss: 0.5509 - acc: 0.6963 - val_loss: 0.6776 - val_acc: 0.5933\n",
      "Epoch 4/30\n",
      " - 330s - loss: 0.5355 - acc: 0.7119 - val_loss: 0.6168 - val_acc: 0.6416\n",
      "Epoch 5/30\n",
      " - 329s - loss: 0.5205 - acc: 0.7241 - val_loss: 0.6179 - val_acc: 0.6427\n",
      "Epoch 6/30\n",
      " - 330s - loss: 0.5131 - acc: 0.7288 - val_loss: 0.6286 - val_acc: 0.6393\n",
      "Epoch 7/30\n",
      " - 328s - loss: 0.5005 - acc: 0.7396 - val_loss: 0.6194 - val_acc: 0.6541\n",
      "Epoch 8/30\n",
      " - 331s - loss: 0.4959 - acc: 0.7454 - val_loss: 0.6527 - val_acc: 0.6273\n",
      "Epoch 9/30\n",
      " - 329s - loss: 0.4899 - acc: 0.7458 - val_loss: 0.6763 - val_acc: 0.6135\n",
      "Epoch 10/30\n",
      " - 329s - loss: 0.4850 - acc: 0.7551 - val_loss: 0.6659 - val_acc: 0.6258\n",
      "Epoch 11/30\n",
      " - 329s - loss: 0.4803 - acc: 0.7602 - val_loss: 0.6367 - val_acc: 0.6488\n",
      "Last training accuracy:0.7601949304887908, validation accuracy: 0.6487950176654261\n",
      "Training on fold 5/5...\n",
      "Found 38769 unique tokens.\n",
      "Found 19465 unique tokens.\n",
      "Shape of X_train_pad tensor: (29549, 770)\n",
      "Shape of y_train tensor: (29549,)\n",
      "Shape of X_val_pad tensor: (7386, 770)\n",
      "Shape of y_val tensor: (7386,)\n",
      "====================\n",
      "Building model...\n",
      "{0: 1.9340882314439063, 1: 0.6743267914194432}\n",
      "Training model...\n",
      "Train on 29549 samples, validate on 7386 samples\n",
      "Epoch 1/30\n",
      " - 328s - loss: 0.6562 - acc: 0.5929 - val_loss: 0.6267 - val_acc: 0.6344\n",
      "Epoch 2/30\n",
      " - 328s - loss: 0.5870 - acc: 0.6708 - val_loss: 0.6565 - val_acc: 0.6124\n",
      "Epoch 3/30\n",
      " - 328s - loss: 0.5509 - acc: 0.7036 - val_loss: 0.7205 - val_acc: 0.5777\n",
      "Epoch 4/30\n",
      " - 329s - loss: 0.5334 - acc: 0.7187 - val_loss: 0.6814 - val_acc: 0.6110\n",
      "Epoch 5/30\n",
      " - 328s - loss: 0.5176 - acc: 0.7334 - val_loss: 0.6629 - val_acc: 0.6350\n",
      "Epoch 6/30\n",
      " - 328s - loss: 0.5081 - acc: 0.7356 - val_loss: 0.6589 - val_acc: 0.6370\n",
      "Epoch 7/30\n",
      " - 329s - loss: 0.5016 - acc: 0.7444 - val_loss: 0.6652 - val_acc: 0.6365\n",
      "Epoch 8/30\n",
      " - 330s - loss: 0.4917 - acc: 0.7515 - val_loss: 0.6547 - val_acc: 0.6492\n",
      "Epoch 9/30\n",
      " - 331s - loss: 0.4881 - acc: 0.7552 - val_loss: 0.6737 - val_acc: 0.6380\n",
      "Epoch 10/30\n",
      " - 331s - loss: 0.4832 - acc: 0.7549 - val_loss: 0.6370 - val_acc: 0.6682\n",
      "Epoch 11/30\n",
      " - 331s - loss: 0.4802 - acc: 0.7601 - val_loss: 0.6646 - val_acc: 0.6462\n",
      "Last training accuracy:0.7601272462890807, validation accuracy: 0.6462225832010781\n"
     ]
    }
   ],
   "source": [
    "## Main program\n",
    "\n",
    "# 5-stratified fold, so classes are balanced\n",
    "skf = StratifiedShuffleSplit(n_splits = 5)\n",
    "num_folds = skf.get_n_splits(X)\n",
    "\n",
    "\n",
    "for index, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"Training on fold \" + str(index + 1) + \"/\" + str(num_folds) + \"...\")\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "#     print(collections.Counter(y_train))\n",
    "\n",
    "    # Create a 2D vector of tokenized words\n",
    "    train_review_lines = [word_tokenize(line) for line in X_train.tolist()]\n",
    "    val_review_lines = [word_tokenize(line) for line in X_val.tolist()]\n",
    "    \n",
    "    # Pad training and validation sequences \n",
    "    X_train_pad, X_val_pad = pad_data(train_review_lines, val_review_lines, MAX_LENGTH)\n",
    "\n",
    "    print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "    print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "    print('Shape of X_val_pad tensor:', X_val_pad.shape)\n",
    "    print('Shape of y_val tensor:', y_val.shape)\n",
    "    print('====================')\n",
    "\n",
    "    # Build network with GRU\n",
    "    model = None\n",
    "    model = create_model(num_words, EMBEDDING_DIM, embedding_matrix, MAX_LENGTH)\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, X_train_pad, y_train, X_val_pad, y_val)\n",
    "\n",
    "    accuracy_history = history.history['acc']\n",
    "    val_accuracy_history = history.history['val_acc']\n",
    "    print(\"Last training accuracy:\" + str(accuracy_history[-1]) + \", validation accuracy: \" + str(val_accuracy_history[-1]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_index(word2vec_model, word2vec_path):\n",
    "    # Save word2vec model\n",
    "    word2vec_model.wv.save_word2vec_format(word2vec_path, binary = False)\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    # Load word2vec model to get embeddings index\n",
    "    with open(word2vec_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:])\n",
    "            embeddings_index[word] = coeffs\n",
    "    \n",
    "    return embeddings_index;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(EMBEDDING_DIM, train_word_index, embeddings_index):\n",
    "    num_words = len(train_word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        \n",
    "    for word, i in train_word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return num_words, embedding_matrix;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(train_review_lines, val_review_lines, MAX_LENGTH):\n",
    "    # Pad training set\n",
    "    train_tokenizer_obj = Tokenizer()\n",
    "    train_tokenizer_obj.fit_on_texts(train_review_lines)\n",
    "    train_sequences = train_tokenizer_obj.texts_to_sequences(train_review_lines)\n",
    "    train_word_index = train_tokenizer_obj.word_index\n",
    "    print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "    X_train_pad = pad_sequences(train_sequences, maxlen = MAX_LENGTH)\n",
    "    \n",
    "    # Pad validation set\n",
    "    val_tokenizer_obj = Tokenizer()\n",
    "    val_tokenizer_obj.fit_on_texts(val_review_lines)\n",
    "    val_sequences = val_tokenizer_obj.texts_to_sequences(val_review_lines)\n",
    "    val_word_index = val_tokenizer_obj.word_index\n",
    "    print('Found %s unique tokens.' % len(val_word_index))\n",
    "    \n",
    "    X_val_pad = pad_sequences(val_sequences, maxlen = MAX_LENGTH)\n",
    "    \n",
    "    return X_train_pad, X_val_pad;\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_test_sequences(test_review_lines, MAX_LENGTH):\n",
    "    test_tokenizer_obj = Tokenizer()\n",
    "    test_tokenizer_obj.fit_on_texts(test_review_lines)\n",
    "    test_sequences = test_tokenizer_obj.texts_to_sequences(test_review_lines)\n",
    "    \n",
    "    X_test_pad = pad_sequences(test_sequences, maxlen = MAX_LENGTH)\n",
    "    \n",
    "    return X_test_pad;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(data_lines, EMBEDDING_DIM):\n",
    "    model = gensim.models.Word2Vec(sentences = data_lines,\n",
    "                              size = EMBEDDING_DIM,\n",
    "                              #negative = 5,\n",
    "                              window = 5,\n",
    "                              workers = 6,\n",
    "                              min_count = 1)\n",
    "    \n",
    "    words = list(model.wv.vocab)\n",
    "    print('Vocab size: ', len(words))\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words, EMBEDDING_DIM, embedding_matrix, MAX_LENGTH):\n",
    "    print(\"Building model...\")\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                               EMBEDDING_DIM,\n",
    "                               embeddings_initializer = Constant(embedding_matrix),\n",
    "                               input_length = MAX_LENGTH,\n",
    "                               trainable = False)\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    \n",
    "    # Currently 64, 0.4, 0.4\n",
    "    model.add(GRU(units = 64, dropout = 0.4, recurrent_dropout = 0.4))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "\n",
    "#     print('Summary of the built model...')\n",
    "#     print(model.summary())\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train_pad, y_train, X_val_pad, y_val):\n",
    "    # Get class weights and early stopping obj\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(y_train),\n",
    "                                                y_train)\n",
    "    \n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10),\n",
    "            ModelCheckpoint(filepath = 'cv_model_checkpoint.h5', monitor = 'val_loss', save_best_only = True)]\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    result = model.fit(X_train_pad, \n",
    "              y_train, \n",
    "              batch_size = 64,\n",
    "              epochs = 30, \n",
    "              validation_data = (X_val_pad, y_val), \n",
    "              verbose = 2, \n",
    "              callbacks = callbacks, \n",
    "              class_weight = class_weight_dict, \n",
    "              shuffle = True)\n",
    "    \n",
    "    return result;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-035c925c658d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"best/best_model_2019-03-26.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model_file_path = \"best/best_model_2019-03-26.h5\"\n",
    "model.save(model_file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy and AUC of model on unseen test set\n",
    "del model\n",
    "model = load_model(model_file_path )\n",
    "\n",
    "test_review_lines = [word_tokenize(line) for line in X_test.tolist()]\n",
    "X_test_pad = pad_test_sequences(test_review_lines, MAX_LENGTH)\n",
    "\n",
    "# Loss score and accuracy\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size = 128)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))\n",
    "\n",
    "# AUC\n",
    "y_pred = model.predict(x = X_test_pad)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print('Test AUC:', auc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
