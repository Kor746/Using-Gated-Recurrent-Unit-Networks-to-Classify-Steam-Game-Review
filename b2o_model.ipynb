{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk, re, time\n",
    "from langdetect import detect\n",
    "from contractions import get_contractions\n",
    "from sqlalchemy import create_engine\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "import chars2vec\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, GlobalMaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# We download stopwords package\n",
    "# nltk.download('stopwords')\n",
    "contractions = get_contractions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "engine = create_engine('mysql://root:@localhost:3306/steam')\n",
    "# steam_data_query = \"\"\"SELECT url AS reviewid, content, CAST(recommend AS SIGNED) AS recommend\n",
    "#     FROM latest_review\"\"\"\n",
    "train_val_data_query = \"\"\"SELECT gameid, url, CAST(recommend AS SIGNED) AS recommend, hours_2w, hours_all, posttime, updatetime, EAG, compensation, content, initial_release_date, lang \n",
    "FROM latest_review \n",
    "WHERE url NOT IN (SELECT DISTINCT url FROM test_set) \n",
    "AND lang = 'en';\n",
    "\"\"\"\n",
    "\n",
    "test_data_query = \"\"\"SELECT gameid, url, CAST(recommend AS SIGNED) AS recommend, hours_2w, hours_all, posttime, updatetime, EAG, compensation, content, initial_release_date, lang\n",
    "FROM test_set;\"\"\"\n",
    "\n",
    "df_train_val = pd.read_sql(train_val_data_query, engine)\n",
    "df_test = pd.read_sql(test_data_query, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53764"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total English Steam reviews\n",
    "len(df_train_val) + len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lines = []\n",
    "\n",
    "lines = df_train_val['content'].values.tolist()\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    new_text = []\n",
    "    for word in tokens:\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    text = \" \".join(new_text)\n",
    "    # remove punctuation from each word    \n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48388\n"
     ]
    }
   ],
   "source": [
    "print(len(review_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65500\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, \n",
    "                               window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'recent_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', 0.7361174821853638),\n",
       " ('awesome', 0.7357403635978699),\n",
       " ('hate', 0.7297787666320801),\n",
       " ('loved', 0.6969842910766602),\n",
       " ('adore', 0.6915671229362488),\n",
       " ('wonderful', 0.6890279650688171),\n",
       " ('liked', 0.6842567920684814),\n",
       " ('enjoy', 0.6815904378890991),\n",
       " ('fantastic', 0.6786534190177917),\n",
       " ('great', 0.6610466241836548)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv.most_similar('bad')#, topn =1)\n",
    "model.wv.similar_by_word(\"love\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'recent_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48388\n",
      "38710\n",
      "9678\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train_val.sample(frac = 0.8)['content']\n",
    "y_train = df_train_val.loc[X_train.index]['recommend']\n",
    "\n",
    "X_val = df_train_val.drop(X_train.index)['content']\n",
    "y_val = df_train_val.loc[X_val.index]['recommend']\n",
    "\n",
    "print(len(df_train_val))\n",
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_val = X_val.values\n",
    "y_val = y_val.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38710,)\n",
      "(9678,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = np.concatenate((X_train, X_val), axis = 0)\n",
    "max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65500 unique tokens.\n",
      "Shape of review tensor: (48388, 1511)\n",
      "Shape of sentiment tensor: (48388,)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df_train_val['recommend'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_val_pad = review_pad[-num_validation_samples:]\n",
    "y_val = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (38711, 1511)\n",
      "Shape of y_train tensor: (38711,)\n",
      "Shape of X_test_pad tensor: (9677, 1511)\n",
      "Shape of y_test tensor: (9677,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_val_pad.shape)\n",
    "print('Shape of y_test tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65501\n"
     ]
    }
   ],
   "source": [
    "print(num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1511, 128)         8384128   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                15456     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,399,617\n",
      "Trainable params: 15,489\n",
      "Non-trainable params: 8,384,128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2.04322812 0.66199808]\n",
      "{0: 2.0432281220310355, 1: 0.661998084684315}\n"
     ]
    }
   ],
   "source": [
    "# Class weights and early stopping\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(y_train),\n",
    "                                                y_train)\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(np.unique(y_train))\n",
    "print(class_weights)\n",
    "print(class_weight_dict)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10),\n",
    "            ModelCheckpoint(filepath = 'checkpoint_model_recent.h5', monitor = 'val_loss', save_best_only = True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 38711 samples, validate on 9677 samples\n",
      "Epoch 1/25\n",
      " - 514s - loss: 0.5344 - acc: 0.7148 - val_loss: 0.4903 - val_acc: 0.7572\n",
      "Epoch 2/25\n",
      " - 503s - loss: 0.4526 - acc: 0.7763 - val_loss: 0.4504 - val_acc: 0.7788\n",
      "Epoch 3/25\n",
      " - 478s - loss: 0.4332 - acc: 0.7857 - val_loss: 0.4178 - val_acc: 0.8038\n",
      "Epoch 4/25\n",
      " - 485s - loss: 0.4188 - acc: 0.7995 - val_loss: 0.3943 - val_acc: 0.8145\n",
      "Epoch 5/25\n",
      " - 501s - loss: 0.4102 - acc: 0.8018 - val_loss: 0.4103 - val_acc: 0.8042\n",
      "Epoch 6/25\n",
      " - 544s - loss: 0.4039 - acc: 0.8056 - val_loss: 0.3656 - val_acc: 0.8318\n",
      "Epoch 7/25\n",
      " - 525s - loss: 0.3973 - acc: 0.8098 - val_loss: 0.3849 - val_acc: 0.8179\n",
      "Epoch 8/25\n",
      " - 528s - loss: 0.3926 - acc: 0.8125 - val_loss: 0.3820 - val_acc: 0.8221\n",
      "Epoch 9/25\n",
      " - 500s - loss: 0.3883 - acc: 0.8151 - val_loss: 0.3783 - val_acc: 0.8237\n",
      "Epoch 10/25\n",
      " - 490s - loss: 0.3827 - acc: 0.8193 - val_loss: 0.3866 - val_acc: 0.8169\n",
      "Epoch 11/25\n",
      " - 488s - loss: 0.3817 - acc: 0.8186 - val_loss: 0.3575 - val_acc: 0.8356\n",
      "Epoch 12/25\n",
      " - 492s - loss: 0.3778 - acc: 0.8219 - val_loss: 0.3699 - val_acc: 0.8279\n",
      "Epoch 13/25\n",
      " - 488s - loss: 0.3771 - acc: 0.8212 - val_loss: 0.3673 - val_acc: 0.8265\n",
      "Epoch 14/25\n",
      " - 499s - loss: 0.3729 - acc: 0.8235 - val_loss: 0.3623 - val_acc: 0.8307\n",
      "Epoch 15/25\n",
      " - 528s - loss: 0.3722 - acc: 0.8244 - val_loss: 0.3740 - val_acc: 0.8233\n",
      "Epoch 16/25\n",
      " - 522s - loss: 0.3672 - acc: 0.8284 - val_loss: 0.3561 - val_acc: 0.8359\n",
      "Epoch 17/25\n",
      " - 537s - loss: 0.3650 - acc: 0.8301 - val_loss: 0.3579 - val_acc: 0.8357\n",
      "Epoch 18/25\n",
      " - 515s - loss: 0.3657 - acc: 0.8272 - val_loss: 0.3730 - val_acc: 0.8267\n",
      "Epoch 19/25\n",
      " - 518s - loss: 0.3607 - acc: 0.8314 - val_loss: 0.3732 - val_acc: 0.8244\n",
      "Epoch 20/25\n",
      " - 516s - loss: 0.3598 - acc: 0.8305 - val_loss: 0.3518 - val_acc: 0.8418\n",
      "Epoch 21/25\n",
      " - 531s - loss: 0.3564 - acc: 0.8337 - val_loss: 0.3525 - val_acc: 0.8396\n",
      "Epoch 22/25\n",
      " - 516s - loss: 0.3577 - acc: 0.8322 - val_loss: 0.3741 - val_acc: 0.8273\n",
      "Epoch 23/25\n",
      " - 532s - loss: 0.3578 - acc: 0.8334 - val_loss: 0.3748 - val_acc: 0.8294\n",
      "Epoch 24/25\n",
      " - 530s - loss: 0.3567 - acc: 0.8355 - val_loss: 0.3765 - val_acc: 0.8279\n",
      "Epoch 25/25\n",
      " - 549s - loss: 0.3526 - acc: 0.8352 - val_loss: 0.3412 - val_acc: 0.8472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3f877a20>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, \n",
    "              y_train, \n",
    "              batch_size = 128,\n",
    "              epochs = 25, \n",
    "              validation_data = (X_val_pad, y_val), \n",
    "              verbose = 2, \n",
    "              callbacks = callbacks, \n",
    "              class_weight = class_weight_dict, \n",
    "              shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    review_lines = []\n",
    "\n",
    "    lines = data['content'].values.tolist()\n",
    "\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        new_text = []\n",
    "        for word in tokens:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "\n",
    "        text = \" \".join(new_text)\n",
    "        # remove punctuation from each word    \n",
    "    #     table = str.maketrans('', '', string.punctuation)\n",
    "    #     stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "    #     words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "        words = word_tokenize(text)\n",
    "        # filter out stop words    \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        review_lines.append(words)\n",
    "    return review_lines;\n",
    "\n",
    "def padding_test(df_test, review_lines, max_length = 1511):\n",
    "\n",
    "    # vectorize the text samples into a 2D integer tensor\n",
    "    tokenizer_obj = Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(review_lines)\n",
    "    sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "\n",
    "    review_pad = pad_sequences(sequences, maxlen = max_length)\n",
    "    sentiment =  df_test['recommend'].values\n",
    "    print('Shape of review tensor:', review_pad.shape)\n",
    "    print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "    # split the data into a training set and a validation set\n",
    " \n",
    "    X_test_pad = review_pad\n",
    "    y_test = sentiment\n",
    "    \n",
    "    return X_test_pad, y_test;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test\n",
    "# df_test['content'].values\n",
    "review_lines = pre_process(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of review tensor: (5376, 1511)\n",
      "Shape of sentiment tensor: (5376,)\n"
     ]
    }
   ],
   "source": [
    "X_test_pad, y_test = padding_test(df_test, review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "5376/5376 [==============================] - 16s 3ms/step\n",
      "Test score: 0.7366692139988854\n",
      "Test accuracy: 0.6155133928571429\n",
      "Accuracy: 61.55%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
