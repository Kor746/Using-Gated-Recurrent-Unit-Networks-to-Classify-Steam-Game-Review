{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from langdetect import detect\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load the Steam review data into Pandas dataframe\n",
    "'''\n",
    "\n",
    "engine = create_engine('mysql://root:@localhost:3306/steam')\n",
    "steam_data_query = \"\"\"SELECT url AS reviewid, content, CAST(recommend AS SIGNED) AS recommend, hours_all, compensation\n",
    "  FROM latest_review \n",
    "\tWHERE content IS NOT NULL \n",
    "\tAND content != ''\n",
    "\tAND content != ' ';\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(steam_data_query, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove non-english reviews, as it could bias our results\n",
    "sentences = df['content']\n",
    "\n",
    "# Create a language column\n",
    "df['lang'] = 'null'\n",
    "\n",
    "# We only want English reviews\n",
    "empty_reviews = []\n",
    "non_english_reviews = []\n",
    "for ind, sentence in sentences.items():\n",
    "    \n",
    "    if sentence == None:\n",
    "        empty_reviews.append(df['reviewid'].iloc[ind])\n",
    "        continue;\n",
    "        \n",
    "    if sentence == '':\n",
    "        empty_reviews.append(df['reviewid'].iloc[ind])\n",
    "        continue;\n",
    "        \n",
    "    if sentence == ' ':\n",
    "        empty_reviews.append(df['reviewid'].iloc[ind])\n",
    "        continue;\n",
    "        \n",
    "    try:\n",
    "        if detect(sentence) == 'en':\n",
    "            language = detect(sentence)\n",
    "            df.at[ind, 'lang'] = language\n",
    "            \n",
    "        else:\n",
    "            non_english_reviews.append(df['reviewid'].iloc[ind])\n",
    "    except:\n",
    "        non_english_reviews.append(df['reviewid'].iloc[ind])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviewid  \\\n",
      "4   http://steamcommunity.com/id/--u/recommended/3...   \n",
      "5   http://steamcommunity.com/id/-7656119804976061...   \n",
      "7   http://steamcommunity.com/id/-Andrealphus-/rec...   \n",
      "9   http://steamcommunity.com/id/-asymmetry/recomm...   \n",
      "10  http://steamcommunity.com/id/-bigboy/recommend...   \n",
      "\n",
      "                                              content  recommend  hours_all  \\\n",
      "4   It's subjectively a compilation of mediocre mi...          0        1.1   \n",
      "5   This game made me lose hope,I played this game...          0      308.0   \n",
      "7          101/101 out of doge coins from Tits Mcguee          1       77.4   \n",
      "9   Extremely visually pleasing with an enticing s...          1        4.4   \n",
      "10                  good game, but with a lot of bugs          1      265.9   \n",
      "\n",
      "   compensation lang  \n",
      "4       b'\\x00'   en  \n",
      "5       b'\\x00'   en  \n",
      "7       b'\\x00'   en  \n",
      "9       b'\\x00'   en  \n",
      "10      b'\\x00'   en  \n"
     ]
    }
   ],
   "source": [
    "data = df[df['lang'] == 'en']\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(compensation):\n",
    "    return int.from_bytes(compensation, 'big');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4     0\n",
      "5     0\n",
      "7     0\n",
      "9     0\n",
      "10    0\n",
      "12    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    1\n",
      "27    0\n",
      "29    0\n",
      "32    0\n",
      "Name: compensation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = data.copy(deep = True)\n",
    "data2['compensation'] = data2['compensation'].map(convert_to_int)\n",
    "print(data2.head(20)['compensation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data2.head(10)['content'])\n",
    "big_text = \"\"\n",
    "for sentence in data2['content'].values.tolist():\n",
    "    big_text += sentence + \" \"\n",
    "big_text = big_text.strip()\n",
    "# big_text = [' '.join(sentence) for sentence in data2['content'].values.tolist()]\n",
    "with open('big_review.txt', 'w') as f:\n",
    "    f.write(big_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import numpy as np\n",
    "import csv\n",
    "import math, collections\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from nltk import tokenize\n",
    "import nltk.data\n",
    "\n",
    "\n",
    "class Sentence_Corrector :\n",
    "    def __init__(self, training_file) :\n",
    "        self.laplaceUnigramCounts = collections.defaultdict(lambda: 0)\n",
    "        self.laplaceBigramCounts = collections.defaultdict(lambda: 0)\n",
    "        self.total = 0\n",
    "        self.sentences = []\n",
    "        self.importantKeywords = set()\n",
    "        self.d = enchant.Dict(\"en_US\")\n",
    "        self.tokenize_file(training_file)\n",
    "        self.train()\n",
    "\n",
    "    def tokenize_file(self, file) :\n",
    "        # \"\"\"\n",
    "        #   Read the file, tokenize and build a list of sentences\n",
    "        # \"\"\"\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        f = open(file)\n",
    "        content = f.read()\n",
    "        for sentence in tokenizer.tokenize(content):\n",
    "            sentence_clean = [i.lower() for i in re.split('[^a-zA-Z]+', sentence) if i]\n",
    "            self.sentences.append(sentence_clean)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # \"\"\"\n",
    "        #   Train unigram and bigram\n",
    "        # \"\"\"\n",
    "        for sentence in self.sentences:\n",
    "            sentence.insert(0, '<s>')\n",
    "            sentence.append('</s>')\n",
    "            for i in range(len(sentence) - 1):\n",
    "                token1 = sentence[i]\n",
    "                token2 = sentence[i + 1]\n",
    "                self.laplaceUnigramCounts[token1] += 1\n",
    "                self.laplaceBigramCounts[(token1, token2)] += 1\n",
    "                self.total += 1\n",
    "            self.total += 1\n",
    "            self.laplaceUnigramCounts[sentence[-1]] += 1\n",
    "\n",
    "\n",
    "    def candidate_word(self, word):\n",
    "        # \"\"\"\n",
    "        # Generate similar word for a given word\n",
    "        # \"\"\"\n",
    "        suggests = []\n",
    "        for candidate in self.importantKeywords:\n",
    "            if candidate.startswith(word):\n",
    "                suggests.append(candidate)\n",
    "        suggests.append(word)\n",
    "\n",
    "        if len(suggests) == 1:\n",
    "            suggests = self.d.suggest(word)\n",
    "            suggests = [suggest.lower() for suggest in suggests][:4]\n",
    "            suggests.append(word)\n",
    "            suggests = list(set(suggests))\n",
    "\n",
    "        return suggests, len(suggests)\n",
    "\n",
    "    def candidate_sentence(self, sentence):\n",
    "        # \"\"\"\n",
    "        # Takes one sentence, and return all the possible sentences, and also return a dictionary of word : suggested number of words\n",
    "        # \"\"\"\n",
    "        candidate_sentences = []\n",
    "        words_count = {}\n",
    "        for word in sentence:\n",
    "            candidate_sentences.append(self.candidate_word(word)[0])\n",
    "            words_count[word] = self.candidate_word(word)[1]\n",
    "\n",
    "        candidate_sentences = list(itertools.product(*candidate_sentences))\n",
    "        return candidate_sentences, words_count\n",
    "\n",
    "    def correction_score(self, words_count, old_sentence, new_sentence) :\n",
    "        # \"\"\"\n",
    "        #   Take a old sentence and a new sentence, for each words in the new sentence, if it's same as the orginal sentence, assign 0.95 prob\n",
    "        #   If it's not same as original sentence, give 0.05 / (count(similarword) - 1)\n",
    "        # \"\"\"\n",
    "        score = 1\n",
    "        for i in range(len(new_sentence)) :\n",
    "            if new_sentence[i] in words_count :\n",
    "                score *= 0.90\n",
    "            else :\n",
    "                score *= (0.05 / (words_count[old_sentence[i]] - 1))\n",
    "        return math.log(score)\n",
    "\n",
    "    def score(self, sentence):\n",
    "        # \"\"\"\n",
    "        #     Takes a list of strings as argument and returns the log-probability of the\n",
    "        #     sentence using the stupid backoff language model.\n",
    "        #     Use laplace smoothing to avoid new words with 0 probability\n",
    "        # \"\"\"\n",
    "        score = 0.0\n",
    "        for i in range(len(sentence) - 1):\n",
    "            if self.laplaceBigramCounts[(sentence[i],sentence[i + 1])] > 0:\n",
    "                score += math.log(self.laplaceBigramCounts[(sentence[i],sentence[i + 1])])\n",
    "                score -= math.log(self.laplaceUnigramCounts[sentence[i]])\n",
    "            else:\n",
    "                score += (math.log(self.laplaceUnigramCounts[sentence[i + 1]] + 1) + math.log(0.4))\n",
    "                score -= math.log(self.total + len(self.laplaceUnigramCounts))\n",
    "        return score\n",
    "\n",
    "    def return_best_sentence(self, old_sentence) :\n",
    "        # \"\"\"\n",
    "        #   Generate all candiate sentences and\n",
    "        #   Calculate the prob of each one and return the one with highest probability\n",
    "        #   Probability involves two part 1. correct probability and 2. language model prob\n",
    "        #   correct prob : p(c | w)\n",
    "        #   language model prob : use stupid backoff algorithm\n",
    "        # \"\"\"\n",
    "        bestScore = float('-inf')\n",
    "        bestSentence = []\n",
    "        old_sentence = [word.lower() for word in old_sentence.split()]\n",
    "        sentences, word_count = self.candidate_sentence(old_sentence)\n",
    "        for new_sentence in sentences:\n",
    "            new_sentence = list(new_sentence)\n",
    "            score = self.correction_score(word_count, new_sentence, old_sentence)\n",
    "            new_sentence.insert(0, '<s>')\n",
    "            new_sentence.append('</s>')\n",
    "            score += self.score(new_sentence)\n",
    "            if score >= bestScore:\n",
    "                bestScore = score\n",
    "                bestSentence = new_sentence\n",
    "        bestSentence = ' '.join(bestSentence[1:-1])\n",
    "        return bestSentence, bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector = Sentence_Corrector('big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('this is wrong appalling world', -38.20485572828627)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector.return_best_sentence('this is wron spallin word')\n",
    "# corrector.return_best_sentence('aoccdrning to a resarch at cmabridge university')\n",
    "# corrector.return_best_sentence('it does not mttaer in waht oredr the ltteers')\n",
    "# corrector.return_best_sentence('the olny important tihng is taht')\n",
    "# corrector.return_best_sentence('hell world')\n",
    "# corrector.return_best_sentence('This used to belong to thew queen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
